import pickle
import torch
import os
import re
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader, Dataset



def align(values_list):
    # (B, T, C+1, H, W) -> (B, T, H, W, C+1)
    B, T, H, W, C_plus_1 = values_list.shape
    C = C_plus_1 - 1

    data = values_list[..., :-1]      # (B, T, H, W, C)
    raw_time = values_list[..., -1]   # (B, T, H, W), assumed in [0, 1)
    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå…ˆ clamp åˆ° [0, 1)ï¼Œå†è½¬ DOY
    raw_time = torch.clamp(raw_time, 0.0, 1.0 - 1e-6)  # é˜²æ­¢ 1.0 å‡ºç°
    doy = torch.floor(raw_time * 365).long() + 1
    doy= doy.permute(0,2,3,1)
    data = data.permute(0,2,3,1,4)
    N = B * H * W
    data_flat = data.reshape(N, T, C)
    doy_flat = doy.reshape(N, T)

    aligned_vals = torch.zeros(N, 365, C, dtype=data.dtype, device=data.device)
    aligned_masks = torch.zeros(N, 365, dtype=torch.float32, device=data.device)
    batch_idx = torch.arange(N, device=data.device).unsqueeze(1).expand(N, T)
    time_idx = doy_flat - 1
    aligned_vals[batch_idx, time_idx] = data_flat
    aligned_masks[batch_idx, time_idx] = 1.0
    # aligned_vals[doy_flat] = data_flat
    return aligned_vals, aligned_masks



class daDataset(Dataset):
    def __init__(self, original_dataloader, missing_ratio=0.1, seed=0, use_index_list=None):
        """
        original_dataloader: åŸå§‹ dataloaderï¼Œè¿”å› {'inputs': (B, T, C+1, H, W)}
        missing_ratio: éšè—æ¯”ä¾‹
        use_index_list: å¯é€‰ï¼Œåªä¿ç•™è¿™äº›æ ·æœ¬ç´¢å¼•ï¼ˆç”¨äº train/valid/test splitï¼‰
        """
        self.samples = []
        self.missing_ratio = missing_ratio
        torch.manual_seed(seed)
        np.random.seed(seed)

        # é¢„åŠ è½½æ‰€æœ‰æ ·æœ¬
        all_samples = []
        for batch in original_dataloader:
            inputs = batch['inputs']
            aligned_vals, observed_masks = align(inputs)
            N, L, C = aligned_vals.shape
            aligned_vals = aligned_vals.cpu().numpy()
            observed_masks = observed_masks.cpu().numpy()
            for i in range(N):
                obs_mask = observed_masks[i]
                gt_mask = self._create_gt_mask(obs_mask, missing_ratio)
                all_samples.append({
                    'observed_data': aligned_vals[i],
                    'observed_mask': observed_masks[i],
                    'gt_mask': gt_mask,
                    'timepoints': np.arange(365)
                })

        # å¦‚æœæŒ‡å®šäº† use_index_listï¼Œåˆ™åªä¿ç•™è¿™äº›
        if use_index_list is not None:
            self.samples = [all_samples[i] for i in use_index_list]
        else:
            self.samples = all_samples

    def _create_gt_mask(self, observed_mask, missing_ratio):
        obs_indices = np.where(observed_mask)[0]
        n_hide = int(len(obs_indices) * missing_ratio)
        if n_hide == 0:
            return observed_mask.copy()
        hide_indices = np.random.choice(obs_indices, n_hide, replace=False)
        gt_mask = observed_mask.copy()
        gt_mask[hide_indices] = 0.0
        return gt_mask

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        item = self.samples[idx]
        K = item['observed_data'].shape[1]
        obs_mask = np.repeat(item['observed_mask'][:, None], K, axis=1)
        gt_mask = np.repeat(item['gt_mask'][:, None], K, axis=1)
        return {
            "observed_data": torch.tensor(item['observed_data'], dtype=torch.float32),
            "observed_mask": torch.tensor(obs_mask, dtype=torch.float32),
            "gt_mask": torch.tensor(gt_mask, dtype=torch.float32),
            "timepoints": torch.tensor(item['timepoints'], dtype=torch.long),
        }

    def get_dataloader(seed=1, nfold=None, batch_size=16, missing_ratio=0.1, original_dataloader=None):
        """
        æ„å»º train/valid/test DataLoaderï¼Œæ”¯æŒ 5-fold åˆ’åˆ†ã€‚

        Args:
            seed: éšæœºç§å­
            nfold: int in [0,4]ï¼ŒæŒ‡å®šæµ‹è¯• foldï¼›è‹¥ä¸º Noneï¼Œåˆ™ä¸åˆ† foldï¼ˆå…¨éƒ¨ç”¨äºè®­ç»ƒï¼‰
            batch_size: æ‰¹å¤§å°
            missing_ratio: æ©ç æ¯”ä¾‹
            original_dataloader: åŸå§‹ dataloaderï¼ˆå¿…é¡»æä¾›ï¼ï¼‰

        Returns:
            train_loader, valid_loader, test_loader
        """
        assert original_dataloader is not None, "original_dataloader must be provided!"

        # ç¬¬ä¸€æ¬¡åŠ è½½å…¨éƒ¨æ•°æ®ä»¥è·å–æ€»é•¿åº¦å’Œç´¢å¼•
        full_dataset = daDataset(
            original_dataloader=original_dataloader,
            missing_ratio=missing_ratio,
            seed=seed,
            use_index_list=None  # åŠ è½½å…¨éƒ¨
        )
        total_len = len(full_dataset)
        indlist = np.arange(total_len)

        np.random.seed(seed)
        np.random.shuffle(indlist)

        if nfold is not None and 0 <= nfold <= 4:
            # 5-fold: 20% test
            start = int(nfold * 0.2 * total_len)
            end = int((nfold + 1) * 0.2 * total_len)
            test_index = indlist[start:end]
            remain_index = np.delete(indlist, np.arange(start, end))
        else:
            # ä¸åš fold åˆ’åˆ†ï¼šå…¨éƒ¨ä½œä¸ºè®­ç»ƒï¼ˆå¯é€‰ï¼‰
            test_index = []
            remain_index = indlist

        # åœ¨ remaining ä¸­åˆ’åˆ† train / valid (70% / 30% of remaining â‰ˆ 56% / 24% of total)
        np.random.seed(seed)
        np.random.shuffle(remain_index)
        num_train = int(len(remain_index) * 0.7)
        train_index = remain_index[:num_train]
        valid_index = remain_index[num_train:]

        # åˆ›å»ºä¸‰ä¸ªå­æ•°æ®é›†
        train_dataset = daDataset(
            original_dataloader=original_dataloader,
            missing_ratio=missing_ratio,
            seed=seed,
            use_index_list=train_index.tolist()
        )
        valid_dataset = daDataset(
            original_dataloader=original_dataloader,
            missing_ratio=missing_ratio,
            seed=seed,
            use_index_list=valid_index.tolist()
        )
        test_dataset = daDataset(
            original_dataloader=original_dataloader,
            missing_ratio=missing_ratio,
            seed=seed,
            use_index_list=test_index.tolist()
        )

        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        return train_loader, valid_loader, test_loader